{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/prashantmanghnani/AmazonEDA/blob/main/AmazonDataAnalysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "373a914b",
      "metadata": {
        "id": "373a914b"
      },
      "source": [
        "#   Myanmar  Amazon  Data  Analysis [Q1]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6a5fd4f7",
      "metadata": {
        "id": "6a5fd4f7"
      },
      "source": [
        "![images.png](data:image/png;base64,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)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d9b52c85",
      "metadata": {
        "id": "d9b52c85"
      },
      "source": [
        "## Aim of this project is to gain insight into the sales data of Amazon to understand the different factors that affect sales of the different branches. [DESCRIPTIVE ANALYSIS]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ff9cb47e",
      "metadata": {
        "id": "ff9cb47e"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pymysql\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dfa50248",
      "metadata": {
        "id": "dfa50248"
      },
      "outputs": [],
      "source": [
        "connection=pymysql.connect(host=\"localhost\",user=\"root\",password=\"1054\",database=\"amazonca\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1200dd2d",
      "metadata": {
        "id": "1200dd2d",
        "outputId": "511d4b12-2159-4b41-83e0-e8d2e107e068"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total Records:\n",
            "   total_records\n",
            "0           1000\n"
          ]
        }
      ],
      "source": [
        "query_total_records = \"SELECT COUNT(*) AS total_records FROM amazon;\"\n",
        "total_records = pd.read_sql(query_total_records, connection)\n",
        "print(\"Total Records:\")\n",
        "print(total_records)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3894f02a",
      "metadata": {
        "id": "3894f02a",
        "outputId": "a4fa4516-7a10-4537-c8df-83383bd4d6a1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   total\n",
            "0     17\n"
          ]
        }
      ],
      "source": [
        "query_total_columns =  \"\"\"\n",
        "SELECT COUNT(*) as total\n",
        "FROM INFORMATION_SCHEMA.COLUMNS\n",
        "WHERE TABLE_NAME = 'amazon' AND TABLE_SCHEMA = 'amazonca';\n",
        "\"\"\"\n",
        "total_columns = pd.read_sql(query_total_columns, connection)\n",
        "print(total_columns)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "91de091e",
      "metadata": {
        "id": "91de091e"
      },
      "source": [
        "#### The shape of the data hence is : 1000 X 17"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a9def794",
      "metadata": {
        "scrolled": true,
        "id": "a9def794",
        "outputId": "23562a28-c61e-4bed-cb97-f0753e5e1c62"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Column Information for 'amazon' Table in 'amazonca' Database:\n",
            "                COLUMN_NAME DATA_TYPE IS_NULLABLE COLUMN_DEFAULT\n",
            "0                Invoice ID      text         YES           None\n",
            "1                    Branch      text         YES           None\n",
            "2                      City      text         YES           None\n",
            "3             Customer type      text         YES           None\n",
            "4                    Gender      text         YES           None\n",
            "5              Product line      text         YES           None\n",
            "6                Unit price    double         YES           None\n",
            "7                  Quantity       int         YES           None\n",
            "8                    Tax 5%    double         YES           None\n",
            "9                     Total    double         YES           None\n",
            "10                     Date      text         YES           None\n",
            "11                     Time      text         YES           None\n",
            "12                  Payment      text         YES           None\n",
            "13                     cogs    double         YES           None\n",
            "14  gross margin percentage    double         YES           None\n",
            "15             gross income    double         YES           None\n",
            "16                   Rating    double         YES           None\n"
          ]
        }
      ],
      "source": [
        "query_column_info = \"\"\"\n",
        "SELECT COLUMN_NAME, DATA_TYPE, IS_NULLABLE, COLUMN_DEFAULT\n",
        "FROM INFORMATION_SCHEMA.COLUMNS\n",
        "WHERE TABLE_NAME = 'amazon' AND TABLE_SCHEMA = 'amazonca';\n",
        "\"\"\"\n",
        "\n",
        "column_info = pd.read_sql(query_column_info, connection)\n",
        "\n",
        "# Display the column information\n",
        "print(\"Column Information for 'amazon' Table in 'amazonca' Database:\")\n",
        "print(column_info)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "824c6919",
      "metadata": {
        "id": "824c6919"
      },
      "source": [
        "#### This dataset contains sales transactions from three different branches of Amazon, respectively located in Mandalay, Yangon and Naypyitaw. The data contains 17 columns and 1000 rows\n",
        "\n",
        "The populations of each city are:\n",
        "\n",
        "- A Yangon: Approximately 5.4 million people.\n",
        "- B Mandalay: Around 1.2 million people.\n",
        "- C Naypyitaw: Roughly 1 million people."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b7f1932f",
      "metadata": {
        "scrolled": true,
        "id": "b7f1932f",
        "outputId": "db1551d6-85b4-4c6a-ace7-de02c386c63e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Columns have been renamed and modified successfully.\n"
          ]
        }
      ],
      "source": [
        "query_rename = \"\"\"\n",
        "ALTER TABLE amazon\n",
        "  RENAME COLUMN `Branch` TO branch,\n",
        "  RENAME COLUMN `City` TO city,\n",
        "  RENAME COLUMN `Customer type` TO customer_type,\n",
        "  RENAME COLUMN `Invoice ID` TO invoice_id,\n",
        "  RENAME COLUMN `Product line` TO product_line,\n",
        "  RENAME COLUMN `Unit price` TO unit_price,\n",
        "  RENAME COLUMN `Tax 5%` TO vat,\n",
        "  RENAME COLUMN `gross income` TO gross_income,\n",
        "  RENAME COLUMN `gross margin percentage` TO gross_margin_percentage;\n",
        "\"\"\"\n",
        "\n",
        "query_modify = \"\"\"\n",
        "ALTER TABLE amazon\n",
        "  MODIFY COLUMN invoice_id VARCHAR(30),\n",
        "  MODIFY COLUMN branch VARCHAR(5),\n",
        "  MODIFY COLUMN city VARCHAR(30),\n",
        "  MODIFY COLUMN customer_type VARCHAR(30),\n",
        "  MODIFY COLUMN gender VARCHAR(10),\n",
        "  MODIFY COLUMN product_line VARCHAR(100),\n",
        "  MODIFY COLUMN unit_price DECIMAL(10, 2),\n",
        "  MODIFY COLUMN quantity INT,\n",
        "  MODIFY COLUMN vat FLOAT(6, 4),\n",
        "  MODIFY COLUMN total DECIMAL(10, 2),\n",
        "  MODIFY COLUMN date DATE,\n",
        "  MODIFY COLUMN time TIMESTAMP,\n",
        "  MODIFY COLUMN payment DECIMAL(10, 2),\n",
        "  MODIFY COLUMN cogs DECIMAL(10, 2),\n",
        "  MODIFY COLUMN gross_margin_percentage FLOAT(11, 9),\n",
        "  MODIFY COLUMN gross_income DECIMAL(10, 2),\n",
        "  MODIFY COLUMN rating FLOAT(2, 1);\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "# Execute the SQL commands to update the table structure\n",
        "with connection.cursor() as cursor:\n",
        "    cursor.execute(query_rename)\n",
        "    cursor.execute(query_modify)\n",
        "    connection.commit()\n",
        "\n",
        "print(\"Columns have been renamed and modified successfully.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9953cc44",
      "metadata": {
        "id": "9953cc44"
      },
      "source": [
        "#### Renaming and changing the datatypes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b7ec2352",
      "metadata": {
        "id": "b7ec2352",
        "outputId": "9847d7c3-0e76-4ff3-a0b2-a0620b0f5c12"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Count of Null Values in Each Column:\n",
            "   branch_nulls  city_nulls  cogs_nulls  customer_type_nulls  date_nulls  \\\n",
            "0           0.0         0.0         0.0                  0.0         0.0   \n",
            "\n",
            "   gender_nulls  gross_income_nulls  gross_margin_percentage_nulls  \\\n",
            "0           0.0                 0.0                            0.0   \n",
            "\n",
            "   invoice_id_nulls  payment_nulls  product_line_nulls  quantity_nulls  \\\n",
            "0               0.0            0.0                 0.0             0.0   \n",
            "\n",
            "   rating_nulls  time_nulls  total_nulls  unit_price_nulls  vat_nulls  \n",
            "0           0.0         0.0          0.0               0.0        0.0  \n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "# SQL query to count null values across all columns in the dataset\n",
        "query_count_nulls = \"\"\"\n",
        "SELECT\n",
        "  SUM(CASE WHEN branch IS NULL THEN 1 ELSE 0 END) AS branch_nulls,\n",
        "  SUM(CASE WHEN city IS NULL THEN 1 ELSE 0 END) AS city_nulls,\n",
        "  SUM(CASE WHEN cogs IS NULL THEN 1 ELSE 0 END) AS cogs_nulls,\n",
        "  SUM(CASE WHEN customer_type IS NULL THEN 1 ELSE 0 END) AS customer_type_nulls,\n",
        "  SUM(CASE WHEN Date IS NULL THEN 1 ELSE 0 END) AS date_nulls,\n",
        "  SUM(CASE WHEN Gender IS NULL THEN 1 ELSE 0 END) AS gender_nulls,\n",
        "  SUM(CASE WHEN gross_income IS NULL THEN 1 ELSE 0 END) AS gross_income_nulls,\n",
        "  SUM(CASE WHEN gross_margin_percentage IS NULL THEN 1 ELSE 0 END) AS gross_margin_percentage_nulls,\n",
        "  SUM(CASE WHEN invoice_id IS NULL THEN 1 ELSE 0 END) AS invoice_id_nulls,\n",
        "  SUM(CASE WHEN Payment IS NULL THEN 1 ELSE 0 END) AS payment_nulls,\n",
        "  SUM(CASE WHEN product_line IS NULL THEN 1 ELSE 0 END) AS product_line_nulls,\n",
        "  SUM(CASE WHEN Quantity IS NULL THEN 1 ELSE 0 END) AS quantity_nulls,\n",
        "  SUM(CASE WHEN Rating IS NULL THEN 1 ELSE 0 END) AS rating_nulls,\n",
        "  SUM(CASE WHEN Time IS NULL THEN 1 ELSE 0 END) AS time_nulls,\n",
        "  SUM(CASE WHEN Total IS NULL THEN 1 ELSE 0 END) AS total_nulls,\n",
        "  SUM(CASE WHEN unit_price IS NULL THEN 1 ELSE 0 END) AS unit_price_nulls,\n",
        "  SUM(CASE WHEN vat IS NULL THEN 1 ELSE 0 END) AS vat_nulls\n",
        "FROM\n",
        "  amazonca.amazon\n",
        "\"\"\"\n",
        "\n",
        "# Retrieve the count of null values across the dataset\n",
        "null_counts_df = pd.read_sql(query_count_nulls, connection)\n",
        "\n",
        "# Display the count of null values for each column\n",
        "print(\"Count of Null Values in Each Column:\")\n",
        "print(null_counts_df)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4365346c",
      "metadata": {
        "id": "4365346c"
      },
      "source": [
        "#### Checking for null values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "04b7559a",
      "metadata": {
        "id": "04b7559a",
        "outputId": "838c06ed-22f0-4306-f32a-8fd876a5d744"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Updated 'amazon' Table with New Columns:\n",
            "    invoice_id branch       city customer_type  Gender  \\\n",
            "0  750-67-8428      A     Yangon        Member  Female   \n",
            "1  226-31-3081      C  Naypyitaw        Normal  Female   \n",
            "\n",
            "             product_line  unit_price  Quantity      vat     Total  \\\n",
            "0       Health and beauty       74.69         7  26.1415  548.9715   \n",
            "1  Electronic accessories       15.28         5   3.8200   80.2200   \n",
            "\n",
            "         Date      Time  Payment    cogs  gross_margin_percentage  \\\n",
            "0  2019-01-05  13:08:00  Ewallet  522.83                 4.761905   \n",
            "1  2019-03-08  10:29:00     Cash   76.40                 4.761905   \n",
            "\n",
            "   gross_income  Rating  timeofday dayname monthname  \n",
            "0       26.1415     9.1  Afternoon     Sat       Jan  \n",
            "1        3.8200     9.6    Morning     Fri       Mar  \n"
          ]
        }
      ],
      "source": [
        "# Query to add new columns\n",
        "query_add_columns = \"\"\"\n",
        "ALTER TABLE amazon\n",
        "  ADD COLUMN timeofday VARCHAR(10) DEFAULT 'Unknown',\n",
        "  ADD COLUMN dayname VARCHAR(10) DEFAULT 'Unknown',\n",
        "  ADD COLUMN monthname VARCHAR(10) DEFAULT 'Unknown';\n",
        "\"\"\"\n",
        "\n",
        "# Query to update 'timeofday'\n",
        "query_update_timeofday = \"\"\"\n",
        "UPDATE amazon\n",
        "SET timeofday =\n",
        "  CASE\n",
        "    WHEN HOUR(time) BETWEEN 6 AND 11 THEN 'Morning'\n",
        "    WHEN HOUR(time) BETWEEN 12 AND 17 THEN 'Afternoon'\n",
        "    WHEN HOUR(time) BETWEEN 18 AND 22 THEN 'Evening'\n",
        "    ELSE 'Night'\n",
        "  END;\n",
        "\"\"\"\n",
        "\n",
        "# Query to update 'dayname'\n",
        "query_update_dayname = \"\"\"\n",
        "UPDATE amazon\n",
        "SET dayname =\n",
        "  CASE\n",
        "    WHEN DAYOFWEEK(date) = 1 THEN 'Sun'\n",
        "    WHEN DAYOFWEEK(date) = 2 THEN 'Mon'\n",
        "    WHEN DAYOFWEEK(date) = 3 THEN 'Tue'\n",
        "    WHEN DAYOFWEEK(date) = 4 THEN 'Wed'\n",
        "    WHEN DAYOFWEEK(date) = 5 THEN 'Thu'\n",
        "    WHEN DAYOFWEEK(date) = 6 THEN 'Fri'\n",
        "    WHEN DAYOFWEEK(date) = 7 THEN 'Sat'\n",
        "  END;\n",
        "\"\"\"\n",
        "\n",
        "# Query to update 'monthname'\n",
        "query_update_monthname = \"\"\"\n",
        "UPDATE amazon\n",
        "SET monthname =\n",
        "  CASE\n",
        "    WHEN MONTH(date) = 1 THEN 'Jan'\n",
        "    WHEN MONTH(date) = 2 THEN 'Feb'\n",
        "    WHEN MONTH(date) = 3 THEN 'Mar'\n",
        "    WHEN MONTH(date) = 4 THEN 'Apr'\n",
        "    WHEN MONTH(date) = 5 THEN 'May'\n",
        "    WHEN MONTH(date) = 6 THEN 'Jun'\n",
        "    WHEN MONTH(date) = 7 THEN 'Jul'\n",
        "    WHEN MONTH(date) = 8 THEN 'Aug'\n",
        "    WHEN MONTH(date) = 9 THEN 'Sep'\n",
        "    WHEN MONTH(date) = 10 THEN 'Oct'\n",
        "    WHEN MONTH(date) = 11 THEN 'Nov'\n",
        "    WHEN MONTH(date) = 12 THEN 'Dec'\n",
        "  END;\n",
        "\"\"\"\n",
        "\n",
        "# Execute the queries to add and update columns\n",
        "with connection.cursor() as cursor:\n",
        "\n",
        "    cursor.execute(query_add_columns)\n",
        "\n",
        "\n",
        "    cursor.execute(query_update_timeofday)\n",
        "\n",
        "\n",
        "    cursor.execute(query_update_dayname)\n",
        "\n",
        "\n",
        "    cursor.execute(query_update_monthname)\n",
        "\n",
        "# Now, retrieve the updated data with pd.read_sql()\n",
        "fetch_query = \"SELECT * FROM amazon\"\n",
        "updated_dataframe = pd.read_sql(fetch_query, connection)\n",
        "\n",
        "# Display the updated DataFrame\n",
        "print(\"Updated 'amazon' Table with New Columns:\")\n",
        "print(updated_dataframe.head(2))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e82da741",
      "metadata": {
        "id": "e82da741"
      },
      "source": [
        "#### Adding three new columns: Month name, Time of day, Day name [Feature Engineering]\n",
        "\n",
        "#### Final Shape is 1000 X 20 [We can futhure normalise the data if required]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c186fe62",
      "metadata": {
        "id": "c186fe62"
      },
      "source": [
        "### Product Analysis\n",
        "\n",
        "Conduct analysis on the data to understand the different product lines, the products lines performing best and the product lines that need to be improved.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "26362bed",
      "metadata": {
        "id": "26362bed",
        "outputId": "d19d142f-392a-4272-e1c0-8cade116adb0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "             product_line  total_sales  total_quantity  average_price  \\\n",
            "0      Food and beverages   56144.8440           952.0      56.008851   \n",
            "1       Sports and travel   55122.8265           920.0      56.993253   \n",
            "2  Electronic accessories   54337.5315           971.0      53.551588   \n",
            "3     Fashion accessories   54305.8950           902.0      57.153652   \n",
            "4      Home and lifestyle   53861.9130           911.0      55.316937   \n",
            "5       Health and beauty   49193.7390           854.0      54.854474   \n",
            "\n",
            "   avg_gross_income  avg_rating  \n",
            "0         15.365310    7.113218  \n",
            "1         15.812630    6.916265  \n",
            "2         15.220597    6.924706  \n",
            "3         14.528062    7.029213  \n",
            "4         16.030331    6.837500  \n",
            "5         15.411572    7.003289  \n"
          ]
        }
      ],
      "source": [
        "# SQL query to get product analysis data\n",
        "query_product_analysis = \"\"\"\n",
        "SELECT\n",
        "    product_line,\n",
        "    SUM(total) AS total_sales,\n",
        "    SUM(quantity) AS total_quantity,\n",
        "    AVG(unit_price) AS average_price,\n",
        "    AVG(gross_income) AS avg_gross_income,\n",
        "    AVG(rating) AS avg_rating\n",
        "FROM amazon\n",
        "GROUP BY product_line\n",
        "ORDER BY total_sales DESC\n",
        "\"\"\"\n",
        "\n",
        "# Execute the query and convert the result to a pandas DataFrame\n",
        "product_analysis_df = pd.read_sql(query_product_analysis, connection)\n",
        "print(product_analysis_df)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1d9ee57d",
      "metadata": {
        "id": "1d9ee57d"
      },
      "source": [
        "Here are the key insights:\n",
        "\n",
        "\n",
        "- **Product Line with Highest Total Sales**: Food and Beverages leads with the highest total sales of 56,144.8440. This suggests strong customer demand and overall sales performance in this category.\n",
        "\n",
        "- **Product Line with Highest Total Quantity Sold**: Despite not having the highest total sales, Electronic Accessories leads in total quantity sold (971.0), indicating a high volume of sales with lower average price points.\n",
        "\n",
        "- **Product Line with Lower Total Sales**: Health and Beauty has the lowest total sales (49,193.7390). This could suggest a need for marketing enhancements or product improvements to boost revenue.\n",
        "\n",
        "- **Product Line with Lowest Average Gross Income**: Fashion Accessories has the lowest average gross income (14.528062), indicating lower profitability. This might necessitate a review of production costs or pricing strategies.\n",
        "\n",
        "- **Strong Performance with High Average Price**: Sports and Travel demonstrates a high total sales and gross income, along with one of the highest average prices (56.993253). This suggests a premium pricing strategy that leads to consistent performance.\n",
        "\n",
        "These insights can guide strategies to address pricing and profitability, improve marketing for lower-performing product lines, and capitalize on trends in high-performing categories."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cf21af44",
      "metadata": {
        "id": "cf21af44"
      },
      "source": [
        "### Sales Analysis\n",
        "\n",
        "This analysis aims to answer the question of the sales trends of product. The result of this can help us measure the effectiveness of each sales strategy the business applies and what modifications are needed to gain more sales.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6799b5b0",
      "metadata": {
        "id": "6799b5b0",
        "outputId": "7c35c58b-3875-406f-a217-1c857648e96e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  monthname  total_sales  total_transactions\n",
            "0       Jan   116291.868                 352\n",
            "1       Mar   109455.507                 345\n",
            "2       Feb    97219.374                 303\n"
          ]
        }
      ],
      "source": [
        "# SQL query to get sales analysis data by month name\n",
        "query_sales_analysis_month_name = \"\"\"\n",
        "SELECT\n",
        "    monthname,\n",
        "    SUM(total) AS total_sales,\n",
        "    COUNT(invoice_id) AS total_transactions\n",
        "FROM amazon\n",
        "GROUP BY monthname\n",
        "ORDER BY total_sales DESC\n",
        "\"\"\"\n",
        "\n",
        "# Execute the query and convert the result to a pandas DataFrame\n",
        "sales_analysis_df = pd.read_sql(query_sales_analysis_month_name, connection)\n",
        "\n",
        "# Display the DataFrame with the month name\n",
        "print(sales_analysis_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "96a4570d",
      "metadata": {
        "id": "96a4570d",
        "outputId": "6d1eda75-7f27-4441-c9c5-a6275e512f9b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   timeofday  total_sales  total_transactions\n",
            "0  Afternoon  172468.5585                 528\n",
            "1    Evening   88699.3800                 281\n",
            "2    Morning   61798.8105                 191\n"
          ]
        }
      ],
      "source": [
        "query_time_of_day_trends = \"\"\"\n",
        "SELECT\n",
        "    timeofday,\n",
        "    SUM(total) AS total_sales,\n",
        "    COUNT(invoice_id) AS total_transactions\n",
        "FROM amazon\n",
        "GROUP BY timeofday\n",
        "ORDER BY total_sales DESC\n",
        "\"\"\"\n",
        "\n",
        "time_of_day_trends_df = pd.read_sql(query_time_of_day_trends, connection)\n",
        "print(time_of_day_trends_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b6ad5b62",
      "metadata": {
        "id": "b6ad5b62",
        "outputId": "f15794b3-daa7-4c9f-9270-c1ea8763c320"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  dayname  total_sales  total_transactions\n",
            "0     Sat   56120.8095                 164\n",
            "1     Tue   51482.2455                 158\n",
            "2     Thu   45349.2480                 138\n",
            "3     Sun   44457.8925                 133\n",
            "4     Fri   43926.3405                 139\n",
            "5     Wed   43731.1350                 143\n",
            "6     Mon   37899.0780                 125\n"
          ]
        }
      ],
      "source": [
        "query_dayname_trends = \"\"\"\n",
        "SELECT\n",
        "    dayname,\n",
        "    SUM(total) AS total_sales,\n",
        "    COUNT(invoice_id) AS total_transactions\n",
        "FROM amazon\n",
        "GROUP BY dayname\n",
        "ORDER BY total_sales DESC\n",
        "\"\"\"\n",
        "\n",
        "dayname_trends_df = pd.read_sql(query_dayname_trends, connection)\n",
        "print(dayname_trends_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "74dc51e2",
      "metadata": {
        "id": "74dc51e2"
      },
      "source": [
        "Here are the key insights:\n",
        "\n",
        "- **January Leads in Sales**: The month with the highest total sales is January, suggesting it is a peak period for customer spending and promotions.\n",
        "\n",
        "- **Afternoon is Prime Time**: Sales and transactions are most frequent in the afternoon, indicating a preference for shopping during this time of day.\n",
        "\n",
        "- **Saturday Tops Weekly Sales**: Saturday has the highest sales and transactions among the days of the week, suggesting it's the best day for marketing and sales efforts.\n",
        "\n",
        "- **Evening Sales Outpace Morning**: Sales in the evening surpass those in the morning, hinting at a trend toward later shopping hours.\n",
        "\n",
        "- **Steady Weekday Sales**: Sales across Tuesday to Thursday are relatively consistent, suggesting a stable flow of weekday business.\n",
        "\n",
        "These insights can help optimize marketing strategies, store hours, and staffing levels to align with customer behavior and maximize sales."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ac992939",
      "metadata": {
        "id": "ac992939"
      },
      "source": [
        "### Customer Analysis\n",
        "\n",
        "This analysis aims to uncover the different customer segments, purchase trends and the profitability of each customer segment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4822173a",
      "metadata": {
        "id": "4822173a",
        "outputId": "d608a9f9-1316-4bd7-9c02-ef116e7dbe2d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  customer_type  total_revenue  avg_gross_income  purchase_count\n",
            "0        Member     164223.444         15.609110             501\n",
            "1        Normal     158743.305         15.148707             499\n"
          ]
        }
      ],
      "source": [
        "# SQL query to get customer analysis data\n",
        "query_customer_analysis = \"\"\"\n",
        "SELECT\n",
        "    customer_type,\n",
        "    SUM(total) AS total_revenue,\n",
        "    AVG(gross_income) AS avg_gross_income,\n",
        "    COUNT(invoice_id) AS purchase_count\n",
        "FROM amazon\n",
        "GROUP BY customer_type\n",
        "ORDER BY total_revenue DESC\n",
        "\"\"\"\n",
        "\n",
        "# Execute the query and convert the result to a pandas DataFrame\n",
        "customer_analysis_df = pd.read_sql(query_customer_analysis, connection)\n",
        "print(customer_analysis_df)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "531efb31",
      "metadata": {
        "id": "531efb31",
        "outputId": "a2967ddd-a301-4d67-b0aa-9c486c66f507"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   gender  total_revenue  avg_gross_income  purchase_count\n",
            "0  Female     167882.925         15.956936             501\n",
            "1    Male     155083.824         14.799487             499\n"
          ]
        }
      ],
      "source": [
        "# SQL query to get customer analysis data grouped by gender\n",
        "query_gender_analysis = \"\"\"\n",
        "SELECT\n",
        "    gender,\n",
        "    SUM(total) AS total_revenue,\n",
        "    AVG(gross_income) AS avg_gross_income,\n",
        "    COUNT(invoice_id) AS purchase_count\n",
        "FROM amazon\n",
        "GROUP BY gender\n",
        "ORDER BY total_revenue DESC\n",
        "\"\"\"\n",
        "\n",
        "# Execute the query and convert the result to a pandas DataFrame\n",
        "gender_analysis_df = pd.read_sql(query_gender_analysis, connection)\n",
        "\n",
        "# Display the DataFrame to inspect the result\n",
        "print(gender_analysis_df)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7344d2fa",
      "metadata": {
        "id": "7344d2fa",
        "outputId": "0ec6b876-a778-41a8-e3ff-8949fe79acef"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Customer Segmentation:\n",
            "  branch       city customer_type  avg_purchase_value  avg_gross_income\n",
            "0      A     Yangon        Member          321.182488         15.294404\n",
            "1      C  Naypyitaw        Normal          337.656755         16.078893\n",
            "2      A     Yangon        Normal          303.831763         14.468179\n",
            "3      B   Mandalay        Member          325.482945         15.499188\n",
            "4      B   Mandalay        Normal          314.329257         14.968060\n",
            "5      C  Naypyitaw        Member          336.575636         16.027411\n"
          ]
        }
      ],
      "source": [
        "# SQL query to get customer segmentation by branch, city, and customer type\n",
        "query_customer_segmentation = \"\"\"\n",
        "SELECT\n",
        "    branch,\n",
        "    city,\n",
        "    customer_type,\n",
        "    AVG(Total) AS avg_purchase_value,\n",
        "    AVG(gross_income) AS avg_gross_income\n",
        "FROM amazon\n",
        "GROUP BY branch, city, customer_type\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "customer_segmentation_df = pd.read_sql(query_customer_segmentation, connection)\n",
        "\n",
        "\n",
        "print(\"Customer Segmentation:\")\n",
        "print(customer_segmentation_df)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "179c9542",
      "metadata": {
        "id": "179c9542",
        "outputId": "85a2fc4f-b9cd-4f81-9748-2712a535b800"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Customer Satisfaction:\n",
            "  customer_type  avg_rating  total_purchases\n",
            "0        Member    6.940319              501\n",
            "1        Normal    7.005210              499\n"
          ]
        }
      ],
      "source": [
        "# SQL query to get customer satisfaction through ratings\n",
        "query_customer_satisfaction = \"\"\"\n",
        "SELECT\n",
        "    customer_type,\n",
        "    AVG(Rating) AS avg_rating,\n",
        "    COUNT(invoice_id) AS total_purchases\n",
        "FROM amazon\n",
        "GROUP BY customer_type\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "customer_satisfaction_df = pd.read_sql(query_customer_satisfaction, connection)\n",
        "\n",
        "print(\"\\nCustomer Satisfaction:\")\n",
        "print(customer_satisfaction_df)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6bfe3ad0",
      "metadata": {
        "id": "6bfe3ad0",
        "outputId": "49a1045a-16af-4cf9-f9ea-ef401fb7144d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Payment Method Analysis:\n",
            "  customer_type      Payment  total_purchases  total_revenue\n",
            "0        Member      Ewallet              161     51790.9560\n",
            "1        Normal         Cash              176     57545.5545\n",
            "2        Normal  Credit card              139     42995.5995\n",
            "3        Normal      Ewallet              184     58202.1510\n",
            "4        Member  Credit card              172     57771.4725\n",
            "5        Member         Cash              168     54661.0155\n"
          ]
        }
      ],
      "source": [
        "# SQL query to analyze payment methods by customer type\n",
        "query_payment_method_analysis = \"\"\"\n",
        "SELECT\n",
        "    customer_type,\n",
        "    Payment,\n",
        "    COUNT(invoice_id) AS total_purchases,\n",
        "    SUM(Total) AS total_revenue\n",
        "FROM amazon\n",
        "GROUP BY customer_type, Payment\n",
        "\"\"\"\n",
        "\n",
        "payment_method_analysis_df = pd.read_sql(query_payment_method_analysis, connection)\n",
        "\n",
        "print(\"\\nPayment Method Analysis:\")\n",
        "print(payment_method_analysis_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fbd7a0ef",
      "metadata": {
        "id": "fbd7a0ef"
      },
      "source": [
        "Insights from the customer analysis data:\n",
        "\n",
        "- **Members Outperform in Revenue**: Members contribute more total revenue than Normal customers, suggesting that loyalty programs may be effective in driving sales.\n",
        "\n",
        "- **Females generate more revenue**: Female customers contribute higher total revenue and have a greater average gross income, suggesting they spend more or shop more frequently.\n",
        "\n",
        "- **Normal Customers Are Slightly Happier**: Normal customers have a higher average rating, suggesting they are generally more satisfied with their experience.\n",
        "\n",
        "- **Naypyitaw Leads in Purchase Value**: Naypyitaw has the highest average purchase value, indicating a potentially lucrative customer base in this city.\n",
        "\n",
        "- **E-Wallets Are a Popular Choice**: Both Member and Normal customers frequently use E-wallets, suggesting it's a preferred payment method.\n",
        "\n",
        "- **Credit Card Popularity Among Members**: Members tend to use credit cards more often, indicating it might be a preferred payment option for this group.\n",
        "\n",
        "\n",
        "These insights can guide marketing efforts, customer satisfaction initiatives, and payment method strategies to better cater to customer preferences and maximize profitability."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bcf93d16",
      "metadata": {
        "id": "bcf93d16"
      },
      "source": [
        "## Business Questions To Answer:"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7468e338",
      "metadata": {
        "id": "7468e338"
      },
      "source": [
        "### 1. What is the count of distinct cities in the dataset?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "27ba1a38",
      "metadata": {
        "id": "27ba1a38",
        "outputId": "132ce803-418c-4663-eb38-c8976dd43664"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Count of Distinct Cities:\n",
            "   distinct_cities_count\n",
            "0                      3\n"
          ]
        }
      ],
      "source": [
        "# SQL query to count distinct cities\n",
        "query_distinct_cities = \"\"\"\n",
        "SELECT COUNT(DISTINCT city) AS distinct_cities_count\n",
        "FROM amazon;\n",
        "\"\"\"\n",
        "\n",
        "# Execute the query and retrieve the result\n",
        "distinct_cities_df = pd.read_sql(query_distinct_cities, connection)\n",
        "print(\"Count of Distinct Cities:\")\n",
        "print(distinct_cities_df)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2ad4ce70",
      "metadata": {
        "id": "2ad4ce70"
      },
      "source": [
        "### 2. For each branch, what is the corresponding city?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "35ca24eb",
      "metadata": {
        "id": "35ca24eb",
        "outputId": "a32b10fc-a2a3-4a87-ef06-a2c64f82c63a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Corresponding City for Each Branch:\n",
            "  branch       city\n",
            "0      A     Yangon\n",
            "1      C  Naypyitaw\n",
            "2      B   Mandalay\n"
          ]
        }
      ],
      "source": [
        "# SQL query to get the corresponding city for each branch\n",
        "query_branch_city = \"\"\"\n",
        "SELECT branch, city\n",
        "FROM amazon\n",
        "GROUP BY branch, city;\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "branch_city_df = pd.read_sql(query_branch_city, connection)\n",
        "print(\"Corresponding City for Each Branch:\")\n",
        "print(branch_city_df)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1bfd9b34",
      "metadata": {
        "id": "1bfd9b34"
      },
      "source": [
        "### 3. What is the count of distinct product lines in the dataset?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5a6af139",
      "metadata": {
        "id": "5a6af139",
        "outputId": "42549896-1297-4b84-a3b8-ae76ca25afae"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Count of Distinct Product Lines:\n",
            "   distinct_product_lines_count\n",
            "0                             6\n"
          ]
        }
      ],
      "source": [
        "# SQL query to count distinct product lines\n",
        "query_distinct_product_lines = \"\"\"\n",
        "SELECT COUNT(DISTINCT product_line) AS distinct_product_lines_count\n",
        "FROM amazon;\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "distinct_product_lines_df = pd.read_sql(query_distinct_product_lines, connection)\n",
        "print(\"Count of Distinct Product Lines:\")\n",
        "print(distinct_product_lines_df)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "858bd8bc",
      "metadata": {
        "id": "858bd8bc"
      },
      "source": [
        "#### Food and beverages,  Sports and travel,   Electronic accessories      \n",
        "#### Fashion accessories,  Home and lifestyle, Health and beauty"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f3b72c9a",
      "metadata": {
        "id": "f3b72c9a"
      },
      "source": [
        "### 4. Which payment method occurs most frequently?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "05aa2d56",
      "metadata": {
        "id": "05aa2d56",
        "outputId": "1320a259-ef7a-47d3-c954-12e833441403"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Most Frequent Payment Method:\n",
            "   payment  frequency\n",
            "0  Ewallet        345\n"
          ]
        }
      ],
      "source": [
        "# SQL query to find the most frequent payment method\n",
        "query_most_frequent_payment = \"\"\"\n",
        "SELECT payment, COUNT(payment) AS frequency\n",
        "FROM amazon\n",
        "GROUP BY payment\n",
        "ORDER BY frequency DESC\n",
        "LIMIT 1;\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "most_frequent_payment_df = pd.read_sql(query_most_frequent_payment, connection)\n",
        "print(\"Most Frequent Payment Method:\")\n",
        "print(most_frequent_payment_df)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1de9c945",
      "metadata": {
        "id": "1de9c945"
      },
      "source": [
        "### 5. Which product line has the highest sales?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dbaed16e",
      "metadata": {
        "id": "dbaed16e",
        "outputId": "037e275c-d2b6-4714-cf5f-1ef4c7c0acef"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Product Line with Highest Sales:\n",
            "         product_line  total_sales\n",
            "0  Food and beverages    56144.844\n"
          ]
        }
      ],
      "source": [
        "# SQL query to find the product line with the highest sales\n",
        "query_highest_sales_product_line = \"\"\"\n",
        "SELECT product_line, SUM(total) AS total_sales\n",
        "FROM amazon\n",
        "GROUP BY product_line\n",
        "ORDER BY total_sales DESC\n",
        "LIMIT 1;\n",
        "\"\"\"\n",
        "\n",
        "highest_sales_product_line_df = pd.read_sql(query_highest_sales_product_line, connection)\n",
        "print(\"Product Line with Highest Sales:\")\n",
        "print(highest_sales_product_line_df)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "775becf3",
      "metadata": {
        "id": "775becf3"
      },
      "source": [
        "\n",
        "### 6. How much revenue is generated each month?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "07303529",
      "metadata": {
        "id": "07303529",
        "outputId": "2a5df9f9-8d71-4241-e3ed-1469d076c613"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Revenue Generated Each Month:\n",
            "  monthname  monthly_revenue\n",
            "0       Jan       116291.868\n",
            "1       Feb        97219.374\n",
            "2       Mar       109455.507\n"
          ]
        }
      ],
      "source": [
        "# SQL query to calculate revenue generated each month\n",
        "query_revenue_per_month = \"\"\"\n",
        "SELECT monthname, SUM(total) AS monthly_revenue\n",
        "FROM amazon\n",
        "GROUP BY monthname\n",
        "ORDER BY monthname\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "revenue_per_month_df = pd.read_sql(query_revenue_per_month, connection)\n",
        "print(\"Revenue Generated Each Month:\")\n",
        "print(revenue_per_month_df)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2b6b3972",
      "metadata": {
        "id": "2b6b3972"
      },
      "source": [
        "### 7. In which month did the cost of goods sold reach its peak?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f199dcb9",
      "metadata": {
        "id": "f199dcb9",
        "outputId": "967cdef9-33f1-48f3-aaf9-fce146a75b9a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Month with Highest COGS:\n",
            "  monthname  total_cogs\n",
            "0       Jan   110754.16\n"
          ]
        }
      ],
      "source": [
        "# SQL query to find the month when COGS peaked\n",
        "query_peak_cogs_month = \"\"\"\n",
        "SELECT monthname, SUM(cogs) AS total_cogs\n",
        "FROM amazon\n",
        "GROUP BY monthname\n",
        "ORDER BY total_cogs DESC\n",
        "LIMIT 1;\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "peak_cogs_month_df = pd.read_sql(query_peak_cogs_month, connection)\n",
        "print(\"Month with Highest COGS:\")\n",
        "print(peak_cogs_month_df)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9ff1b1d2",
      "metadata": {
        "id": "9ff1b1d2"
      },
      "source": [
        "### 8. Which product line generated the highest revenue?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "33ed0b9a",
      "metadata": {
        "id": "33ed0b9a",
        "outputId": "9d036498-3d1e-40fa-998f-3865ac062106"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Product Line with Highest Revenue:\n",
            "         product_line  total_revenue\n",
            "0  Food and beverages      56144.844\n"
          ]
        }
      ],
      "source": [
        "# SQL query to find the product line with the highest revenue\n",
        "query_highest_revenue_product_line = \"\"\"\n",
        "SELECT product_line, SUM(total) AS total_revenue\n",
        "FROM amazon\n",
        "GROUP BY product_line\n",
        "ORDER BY total_revenue DESC\n",
        "LIMIT 1;\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "highest_revenue_product_line_df = pd.read_sql(query_highest_revenue_product_line, connection)\n",
        "print(\"Product Line with Highest Revenue:\")\n",
        "print(highest_revenue_product_line_df)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a1f522ce",
      "metadata": {
        "id": "a1f522ce"
      },
      "source": [
        "### 9. In which city was the highest revenue recorded?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "70041b5c",
      "metadata": {
        "id": "70041b5c",
        "outputId": "cfc7e6a8-35fe-4859-83b4-8b5d17db4b95"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "City with Highest Revenue:\n",
            "        city  total_revenue\n",
            "0  Naypyitaw    110568.7065\n"
          ]
        }
      ],
      "source": [
        "# SQL query to find the city with the highest revenue\n",
        "query_highest_revenue_city = \"\"\"\n",
        "SELECT city, SUM(total) AS total_revenue\n",
        "FROM amazon\n",
        "GROUP BY city\n",
        "ORDER BY total_revenue DESC\n",
        "LIMIT 1;\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "highest_revenue_city_df = pd.read_sql(query_highest_revenue_city, connection)\n",
        "print(\"City with Highest Revenue:\")\n",
        "print(highest_revenue_city_df)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3c7e07a6",
      "metadata": {
        "id": "3c7e07a6"
      },
      "source": [
        "\n",
        "### 10. Which product line incurred the highest Value Added Tax?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "72f9692b",
      "metadata": {
        "id": "72f9692b",
        "outputId": "7fbd3cb7-82b4-46f0-a04f-a48b13fe81ae"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Product Line with Highest VAT:\n",
            "         product_line  total_vat\n",
            "0  Food and beverages   2673.564\n"
          ]
        }
      ],
      "source": [
        "# SQL query to find the product line with the highest VAT\n",
        "query_highest_vat_product_line = \"\"\"\n",
        "SELECT product_line, SUM(vat) AS total_vat\n",
        "FROM amazon\n",
        "GROUP BY product_line\n",
        "ORDER BY total_vat DESC\n",
        "LIMIT 1;\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "highest_vat_product_line_df = pd.read_sql(query_highest_vat_product_line, connection)\n",
        "print(\"Product Line with Highest VAT:\")\n",
        "print(highest_vat_product_line_df)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e11e1806",
      "metadata": {
        "id": "e11e1806"
      },
      "source": [
        "\n",
        "### 11. For each product line, add a column indicating \"Good\" if its sales are above average, otherwise \"Bad.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cead6d37",
      "metadata": {
        "id": "cead6d37",
        "outputId": "3e6cd608-4e44-4aea-abea-ce8d0fdba9e8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Product Lines with 'Good' or 'Bad' Sales:\n",
            "             product_line  total_sales sales_performance\n",
            "0       Health and beauty   49193.7390               Bad\n",
            "1  Electronic accessories   54337.5315              Good\n",
            "2      Home and lifestyle   53861.9130              Good\n",
            "3       Sports and travel   55122.8265              Good\n",
            "4      Food and beverages   56144.8440              Good\n",
            "5     Fashion accessories   54305.8950              Good\n"
          ]
        }
      ],
      "source": [
        "# SQL query to add a new column indicating \"Good\" or \"Bad\" based on average sales\n",
        "query_good_bad_sales = \"\"\"\n",
        "-- Calculate the average sales for product lines\n",
        "WITH avg_sales AS (\n",
        "  SELECT AVG(total_sales) AS avg_total_sales\n",
        "  FROM (\n",
        "    SELECT product_line, SUM(total) AS total_sales\n",
        "    FROM amazon\n",
        "    GROUP BY product_line\n",
        "  ) sub\n",
        ")\n",
        "\n",
        "-- Check if each product line has sales above or below the average\n",
        "SELECT\n",
        "  product_line,\n",
        "  SUM(total) AS total_sales,\n",
        "  CASE\n",
        "    WHEN SUM(total) > (SELECT avg_total_sales FROM avg_sales) THEN 'Good'\n",
        "    ELSE 'Bad'\n",
        "  END AS sales_performance\n",
        "FROM\n",
        "  amazon\n",
        "GROUP BY\n",
        "  product_line\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "good_bad_sales_df = pd.read_sql(query_good_bad_sales, connection)\n",
        "print(\"Product Lines with 'Good' or 'Bad' Sales:\")\n",
        "print(good_bad_sales_df)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "90f62626",
      "metadata": {
        "id": "90f62626"
      },
      "source": [
        "### 12. Identify the branch that exceeded the average number of products sold."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fe0d9678",
      "metadata": {
        "id": "fe0d9678",
        "outputId": "b7cb8c07-e5c6-4b5e-b62f-592afed1b461"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Branch Performance Based on Product Sales:\n",
            "  branch  total_quantity    performance\n",
            "0      A          1859.0  Above Average\n",
            "1      C          1831.0  Below Average\n",
            "2      B          1820.0  Below Average\n"
          ]
        }
      ],
      "source": [
        "# SQL query to find the branch that exceeded the average number of products sold\n",
        "query_exceeding_branch = \"\"\"\n",
        "-- Calculate the average total quantity by branch\n",
        "WITH avg_quantity AS (\n",
        "  SELECT AVG(total_quantity) AS avg_total_quantity\n",
        "  FROM (\n",
        "    SELECT branch, SUM(quantity) AS total_quantity\n",
        "    FROM amazon\n",
        "    GROUP BY branch\n",
        "  ) sub\n",
        ")\n",
        "\n",
        "-- Check if each branch's product sales are above or below the average\n",
        "SELECT\n",
        "  branch,\n",
        "  SUM(quantity) AS total_quantity,\n",
        "  CASE\n",
        "    WHEN SUM(quantity) > (SELECT avg_total_quantity FROM avg_quantity) THEN 'Above Average'\n",
        "    ELSE 'Below Average'\n",
        "  END AS performance\n",
        "FROM\n",
        "  amazon\n",
        "GROUP BY\n",
        "  branch\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "exceeding_branch_df = pd.read_sql(query_exceeding_branch, connection)\n",
        "print(\"Branch Performance Based on Product Sales:\")\n",
        "print(exceeding_branch_df)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a1ba0962",
      "metadata": {
        "id": "a1ba0962"
      },
      "source": [
        "\n",
        "### 13. Which product line is most frequently associated with each gender?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0061de25",
      "metadata": {
        "id": "0061de25",
        "outputId": "c8d09d41-06f0-45f2-d6a7-5aac8091e1b2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Product Line Frequency for Each Gender:\n",
            "    gender            product_line  frequency\n",
            "0   Female     Fashion accessories         96\n",
            "1   Female      Food and beverages         90\n",
            "2   Female       Sports and travel         88\n",
            "3   Female  Electronic accessories         84\n",
            "4   Female      Home and lifestyle         79\n",
            "5   Female       Health and beauty         64\n",
            "6     Male       Health and beauty         88\n",
            "7     Male  Electronic accessories         86\n",
            "8     Male      Food and beverages         84\n",
            "9     Male     Fashion accessories         82\n",
            "10    Male      Home and lifestyle         81\n",
            "11    Male       Sports and travel         78\n"
          ]
        }
      ],
      "source": [
        "# SQL query to find the most frequent product line for each gender\n",
        "query_product_line_per_gender = \"\"\"\n",
        "SELECT gender, product_line, COUNT(product_line) AS frequency\n",
        "FROM amazon\n",
        "GROUP BY gender, product_line\n",
        "ORDER BY gender, frequency DESC;\n",
        "\"\"\"\n",
        "\n",
        "# Execute the query and retrieve the result\n",
        "product_line_per_gender_df = pd.read_sql(query_product_line_per_gender, connection)\n",
        "print(\"Product Line Frequency for Each Gender:\")\n",
        "print(product_line_per_gender_df)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b7a9476a",
      "metadata": {
        "id": "b7a9476a"
      },
      "source": [
        "\n",
        "### 14. Calculate the average rating for each product line."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e1f8e8d9",
      "metadata": {
        "id": "e1f8e8d9",
        "outputId": "8a404afd-a25b-4785-c81f-636d0ab525da"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Average Rating for Each Product Line:\n",
            "             product_line  avg_rating\n",
            "0       Health and beauty    7.003289\n",
            "1  Electronic accessories    6.924706\n",
            "2      Home and lifestyle    6.837500\n",
            "3       Sports and travel    6.916265\n",
            "4      Food and beverages    7.113218\n",
            "5     Fashion accessories    7.029213\n"
          ]
        }
      ],
      "source": [
        "# SQL query to calculate the average rating for each product line\n",
        "query_avg_rating_per_product_line = \"\"\"\n",
        "SELECT product_line, AVG(rating) AS avg_rating\n",
        "FROM amazon\n",
        "GROUP BY product_line;\n",
        "\"\"\"\n",
        "\n",
        "# Execute the query and retrieve the result\n",
        "avg_rating_per_product_line_df = pd.read_sql(query_avg_rating_per_product_line, connection)\n",
        "print(\"Average Rating for Each Product Line:\")\n",
        "print(avg_rating_per_product_line_df)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f934c354",
      "metadata": {
        "id": "f934c354"
      },
      "source": [
        "### 15. Count the sales occurrences for each time of day on every weekday."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ca39a217",
      "metadata": {
        "id": "ca39a217",
        "outputId": "4144efca-7de8-4b83-fcfe-cc588340024a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sales Occurrences for Each Time of Day on Every Weekday:\n",
            "   dayname  timeofday  sales_count\n",
            "0      Mon    Morning           21\n",
            "1      Mon  Afternoon           75\n",
            "2      Mon    Evening           29\n",
            "3      Tue    Morning           36\n",
            "4      Tue  Afternoon           71\n",
            "5      Tue    Evening           51\n",
            "6      Wed    Morning           22\n",
            "7      Wed  Afternoon           81\n",
            "8      Wed    Evening           40\n",
            "9      Thu    Morning           33\n",
            "10     Thu  Afternoon           76\n",
            "11     Thu    Evening           29\n",
            "12     Fri    Morning           29\n",
            "13     Fri  Afternoon           74\n",
            "14     Fri    Evening           36\n",
            "15     Sat    Morning           28\n",
            "16     Sat  Afternoon           81\n",
            "17     Sat    Evening           55\n",
            "18     Sun    Morning           22\n",
            "19     Sun  Afternoon           70\n",
            "20     Sun    Evening           41\n"
          ]
        }
      ],
      "source": [
        "# SQL query to count sales occurrences for each time of day on every weekday\n",
        "query_sales_per_weekday_time = \"\"\"\n",
        "SELECT dayname, timeofday, COUNT(timeofday) AS sales_count\n",
        "FROM amazon\n",
        "GROUP BY dayname, timeofday\n",
        "ORDER BY dayname, timeofday\n",
        "\"\"\"\n",
        "\n",
        "# Execute the query and retrieve the result\n",
        "sales_per_weekday_time_df = pd.read_sql(query_sales_per_weekday_time, connection)\n",
        "print(\"Sales Occurrences for Each Time of Day on Every Weekday:\")\n",
        "print(sales_per_weekday_time_df)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f89ca28a",
      "metadata": {
        "id": "f89ca28a"
      },
      "source": [
        "### 16. Identify the customer type contributing the highest revenue."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bad85615",
      "metadata": {
        "id": "bad85615",
        "outputId": "9633d6e0-5114-4411-ff4b-a203853f673b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Customer Type Contributing the Highest Revenue:\n",
            "  customer_type  total_revenue\n",
            "0        Member     164223.444\n"
          ]
        }
      ],
      "source": [
        "# SQL query to find the customer type contributing the highest revenue\n",
        "query_highest_revenue_customer_type = \"\"\"\n",
        "SELECT customer_type, SUM(total) AS total_revenue\n",
        "FROM amazon\n",
        "GROUP BY customer_type\n",
        "ORDER BY total_revenue DESC\n",
        "LIMIT 1;\n",
        "\"\"\"\n",
        "\n",
        "# Execute the query\n",
        "highest_revenue_customer_type_df = pd.read_sql(query_highest_revenue_customer_type, connection)\n",
        "print(\"Customer Type Contributing the Highest Revenue:\")\n",
        "print(highest_revenue_customer_type_df)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ce7ed2e1",
      "metadata": {
        "id": "ce7ed2e1"
      },
      "source": [
        "### 17. Determine the city with the highest VAT percentage."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "01bf50ac",
      "metadata": {
        "id": "01bf50ac",
        "outputId": "cd26833a-c6e8-4b3b-e231-48ae4aa24702"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "City with the Highest VAT Percentage:\n",
            "        city    avg_vat\n",
            "0  Naypyitaw  16.052367\n"
          ]
        }
      ],
      "source": [
        "# SQL query to determine the city with the highest VAT percentage\n",
        "query_highest_vat_city = \"\"\"\n",
        "SELECT city, AVG(vat) AS avg_vat\n",
        "FROM amazon\n",
        "GROUP BY city\n",
        "ORDER BY avg_vat DESC\n",
        "LIMIT 1;\n",
        "\"\"\"\n",
        "\n",
        "# Execute the query\n",
        "highest_vat_city_df = pd.read_sql(query_highest_vat_city, connection)\n",
        "print(\"City with the Highest VAT Percentage:\")\n",
        "print(highest_vat_city_df)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "00fcbc0c",
      "metadata": {
        "id": "00fcbc0c"
      },
      "source": [
        "\n",
        "\n",
        "### 18. Identify the customer type with the highest VAT payments.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "276184ff",
      "metadata": {
        "id": "276184ff",
        "outputId": "a0a9689f-0d5f-4d48-c831-813bf5fb3dac"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Customer Type with the Highest VAT Payments:\n",
            "  customer_type  total_vat\n",
            "0        Member   7820.164\n"
          ]
        }
      ],
      "source": [
        "# SQL query to find the customer type with the highest VAT payments\n",
        "query_highest_vat_customer_type = \"\"\"\n",
        "SELECT customer_type, SUM(vat) AS total_vat\n",
        "FROM amazon\n",
        "GROUP BY customer_type\n",
        "ORDER BY total_vat DESC\n",
        "LIMIT 1;\n",
        "\"\"\"\n",
        "\n",
        "# Execute the query\n",
        "highest_vat_customer_type_df = pd.read_sql(query_highest_vat_customer_type, connection)\n",
        "print(\"Customer Type with the Highest VAT Payments:\")\n",
        "print(highest_vat_customer_type_df)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5094e450",
      "metadata": {
        "id": "5094e450"
      },
      "source": [
        "### 19. What is the count of distinct customer types in the dataset?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b9c7f3b7",
      "metadata": {
        "id": "b9c7f3b7",
        "outputId": "29356608-bf69-431d-ee6d-7b0c5c30b559"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Count of Distinct Customer Types:\n",
            "   distinct_customer_types_count\n",
            "0                              2\n"
          ]
        }
      ],
      "source": [
        "# SQL query to count distinct customer types\n",
        "query_distinct_customer_types = \"\"\"\n",
        "SELECT COUNT(DISTINCT customer_type) AS distinct_customer_types_count\n",
        "FROM amazon;\n",
        "\"\"\"\n",
        "\n",
        "# Execute the query\n",
        "distinct_customer_types_df = pd.read_sql(query_distinct_customer_types, connection)\n",
        "print(\"Count of Distinct Customer Types:\")\n",
        "print(distinct_customer_types_df)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "244e9a6b",
      "metadata": {
        "id": "244e9a6b"
      },
      "source": [
        "\n",
        "### 20. What is the count of distinct payment methods in the dataset?\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4b88a8c3",
      "metadata": {
        "id": "4b88a8c3",
        "outputId": "21450873-0f89-4c4a-bda1-3c5cf7746584"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Count of Distinct Payment Methods:\n",
            "   distinct_payment_methods_count\n",
            "0                               3\n"
          ]
        }
      ],
      "source": [
        "# SQL query to count distinct payment methods\n",
        "query_distinct_payment_methods = \"\"\"\n",
        "SELECT COUNT(DISTINCT payment) AS distinct_payment_methods_count\n",
        "FROM amazon;\n",
        "\"\"\"\n",
        "\n",
        "# Execute the query\n",
        "distinct_payment_methods_df = pd.read_sql(query_distinct_payment_methods, connection)\n",
        "print(\"Count of Distinct Payment Methods:\")\n",
        "print(distinct_payment_methods_df)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0c46b2e8",
      "metadata": {
        "id": "0c46b2e8"
      },
      "source": [
        "#### E-wallet, Credit Card, and Cash"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6adf8706",
      "metadata": {
        "id": "6adf8706"
      },
      "source": [
        "### 21. Which customer type occurs most frequently?\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4bd50b7a",
      "metadata": {
        "id": "4bd50b7a",
        "outputId": "b75588e8-4b27-4f6a-f881-ba6d7a9bb59c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Customer Type with the Highest Purchase Frequency:\n",
            "  customer_type  occurs_count\n",
            "0        Member           501\n"
          ]
        }
      ],
      "source": [
        "# SQL query to find the customer type with the highest purchase frequency\n",
        "query_highest_purchase_customer_type = \"\"\"\n",
        "SELECT customer_type, COUNT(*) AS occurs_count\n",
        "FROM amazon\n",
        "GROUP BY customer_type\n",
        "ORDER BY occurs_count DESC\n",
        "LIMIT 1;\n",
        "\"\"\"\n",
        "\n",
        "# Execute the query\n",
        "highest_purchase_customer_type_df = pd.read_sql(query_highest_purchase_customer_type, connection)\n",
        "print(\"Customer Type with the Highest Purchase Frequency:\")\n",
        "print(highest_purchase_customer_type_df)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5fcfe229",
      "metadata": {
        "id": "5fcfe229"
      },
      "source": [
        "### 22. Identify the customer type with the highest purchase frequency."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "657bd604",
      "metadata": {
        "id": "657bd604",
        "outputId": "33524b66-6b97-4866-f4ce-ae2ac6a43644"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Customer Type with the Highest Purchase Frequency:\n",
            "  customer_type  purchase_count\n",
            "0        Member             501\n"
          ]
        }
      ],
      "source": [
        "# SQL query to find the customer type with the highest purchase frequency\n",
        "query_highest_purchase_customer_type = \"\"\"\n",
        "SELECT customer_type, COUNT(*) AS purchase_count\n",
        "FROM amazon\n",
        "GROUP BY customer_type\n",
        "ORDER BY purchase_count DESC\n",
        "LIMIT 1;\n",
        "\"\"\"\n",
        "\n",
        "# Execute the query\n",
        "highest_purchase_customer_type_df = pd.read_sql(query_highest_purchase_customer_type, connection)\n",
        "print(\"Customer Type with the Highest Purchase Frequency:\")\n",
        "print(highest_purchase_customer_type_df)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "770149c7",
      "metadata": {
        "id": "770149c7"
      },
      "source": [
        "### 23. Determine the predominant gender among customers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ff203f3e",
      "metadata": {
        "id": "ff203f3e",
        "outputId": "06a30117-5f38-46da-b610-5bb2e234d44a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Predominant Gender Among Customers:\n",
            "   gender  gender_count\n",
            "0  Female           501\n"
          ]
        }
      ],
      "source": [
        "# SQL query to determine the predominant gender among customers\n",
        "query_predominant_gender = \"\"\"\n",
        "SELECT gender, COUNT(*) AS gender_count\n",
        "FROM amazon\n",
        "GROUP BY gender\n",
        "ORDER BY gender_count DESC\n",
        "LIMIT 1;\n",
        "\"\"\"\n",
        "\n",
        "# Execute the query\n",
        "predominant_gender_df = pd.read_sql(query_predominant_gender, connection)\n",
        "print(\"Predominant Gender Among Customers:\")\n",
        "print(predominant_gender_df)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1854b077",
      "metadata": {
        "id": "1854b077"
      },
      "source": [
        "### 24. Examine the distribution of genders within each branch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5fe0c551",
      "metadata": {
        "id": "5fe0c551",
        "outputId": "6b5bacdd-fee9-4035-eede-9b6c8bdeb3ac"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Gender Distribution Within Each Branch:\n",
            "  branch  gender  gender_count\n",
            "0      A  Female           161\n",
            "1      C  Female           178\n",
            "2      A    Male           179\n",
            "3      C    Male           150\n",
            "4      B  Female           162\n",
            "5      B    Male           170\n"
          ]
        }
      ],
      "source": [
        "# SQL query to examine the distribution of genders within each branch\n",
        "query_gender_distribution_per_branch = \"\"\"\n",
        "SELECT branch, gender, COUNT(*) AS gender_count\n",
        "FROM amazon\n",
        "GROUP BY branch, gender;\n",
        "\"\"\"\n",
        "\n",
        "# Execute the query\n",
        "gender_distribution_per_branch_df = pd.read_sql(query_gender_distribution_per_branch, connection)\n",
        "print(\"Gender Distribution Within Each Branch:\")\n",
        "print(gender_distribution_per_branch_df)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "52f7f6b7",
      "metadata": {
        "id": "52f7f6b7"
      },
      "source": [
        "### 25. Identify the time of day when customers provide the most ratings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "23c58ca0",
      "metadata": {
        "id": "23c58ca0",
        "outputId": "fb8e6c16-feda-469a-c67d-e5269899a870"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Time of Day When Customers Provide the Most Ratings:\n",
            "   timeofday  rating_count\n",
            "0  Afternoon           528\n",
            "1    Evening           281\n",
            "2    Morning           191\n"
          ]
        }
      ],
      "source": [
        "# SQL query to find the time of day when customers provide the most ratings\n",
        "query_ratings_per_timeofday = \"\"\"\n",
        "SELECT timeofday, COUNT(rating) AS rating_count\n",
        "FROM amazon\n",
        "GROUP BY timeofday\n",
        "ORDER BY rating_count DESC;\n",
        "\"\"\"\n",
        "\n",
        "# Execute the query\n",
        "ratings_per_timeofday_df = pd.read_sql(query_ratings_per_timeofday, connection)\n",
        "print(\"Time of Day When Customers Provide the Most Ratings:\")\n",
        "print(ratings_per_timeofday_df)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9d309de3",
      "metadata": {
        "id": "9d309de3"
      },
      "source": [
        "### 26. Determine the time of day with the highest customer ratings for each branch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a0029b7c",
      "metadata": {
        "id": "a0029b7c",
        "outputId": "6de811d2-6591-4839-afab-9354978271d4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Time of Day with the Highest Customer Ratings for Each Branch:\n",
            "  branch timeof_day  avg_rating\n",
            "0      A  Afternoon    7.056757\n",
            "1      B    Morning    6.891525\n",
            "2      C  Afternoon    7.095580\n"
          ]
        }
      ],
      "source": [
        "# SQL query to determine the day of the week with the highest average ratings for each branch\n",
        "query_highest_avg_ratings_per_branch_day = \"\"\"\n",
        "WITH avg_ratings_per_time AS (\n",
        "  SELECT\n",
        "    branch,\n",
        "    timeofday,\n",
        "    AVG(Rating) AS avg_rating\n",
        "  FROM\n",
        "    amazon\n",
        "  GROUP BY\n",
        "    branch, timeofday\n",
        ")\n",
        "\n",
        "SELECT\n",
        "  branch,\n",
        "  timeofday,\n",
        "  avg_rating\n",
        "FROM\n",
        "  avg_ratings_per_time\n",
        "WHERE\n",
        "  (branch, avg_rating) IN (\n",
        "    SELECT\n",
        "      branch,\n",
        "      MAX(avg_rating)\n",
        "    FROM\n",
        "      avg_ratings_per_time\n",
        "    GROUP BY\n",
        "      branch\n",
        "  )\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "\n",
        "# Execute the query and convert the result to a pandas DataFrame\n",
        "highest_ratings_per_branch_time_df = pd.read_sql(query_highest_ratings_per_branch_time, connection)\n",
        "\n",
        "\n",
        "print(\"Time of Day with the Highest Customer Ratings for Each Branch:\")\n",
        "print(highest_ratings_per_branch_time_df)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "64a20491",
      "metadata": {
        "id": "64a20491"
      },
      "source": [
        "### 27. Identify the day of the week with the highest average ratings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "03bca1e1",
      "metadata": {
        "id": "03bca1e1",
        "outputId": "4dcbc4ec-2411-4f8b-c83b-8710b477e308"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Day of the Week with the Highest Average Ratings:\n",
            "  dayname  avg_rating\n",
            "0     Mon    7.153600\n",
            "1     Fri    7.076259\n",
            "2     Sun    7.011278\n",
            "3     Tue    7.003165\n",
            "4     Sat    6.901829\n",
            "5     Thu    6.889855\n",
            "6     Wed    6.805594\n"
          ]
        }
      ],
      "source": [
        "# SQL query to identify the day of the week with the highest average ratings\n",
        "query_highest_avg_ratings_per_day = \"\"\"\n",
        "SELECT dayname, AVG(rating) AS avg_rating\n",
        "FROM amazon\n",
        "GROUP BY dayname\n",
        "ORDER BY avg_rating DESC;\n",
        "\"\"\"\n",
        "\n",
        "# Execute the query\n",
        "highest_avg_ratings_per_day_df = pd.read_sql(query_highest_avg_ratings_per_day, connection)\n",
        "print(\"Day of the Week with the Highest Average Ratings:\")\n",
        "print(highest_avg_ratings_per_day_df)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3411eb60",
      "metadata": {
        "id": "3411eb60"
      },
      "source": [
        "### 28. Determine the day of the week with the highest average ratings for each branch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b89c78ab",
      "metadata": {
        "id": "b89c78ab",
        "outputId": "3427215b-f854-42e4-b399-bf468e751ab5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Time of Day with the Highest Customer Ratings for Each Branch:\n",
            "  branch day_of_week  avg_rating\n",
            "0      C      Friday    7.278947\n",
            "1      A      Friday    7.312000\n",
            "2      B      Monday    7.335897\n"
          ]
        }
      ],
      "source": [
        "# SQL query to determine the time of day with the highest customer ratings for each branch\n",
        "query_highest_ratings_per_branch = \"\"\"\n",
        "WITH avg_ratings_per_day AS (\n",
        "  SELECT\n",
        "    branch,\n",
        "    DAYNAME(Date) AS day_of_week,\n",
        "    AVG(Rating) AS avg_rating\n",
        "  FROM\n",
        "    amazon\n",
        "  GROUP BY\n",
        "    branch, day_of_week\n",
        ")\n",
        "\n",
        "SELECT\n",
        "  branch,\n",
        "  day_of_week,\n",
        "  avg_rating\n",
        "FROM\n",
        "  avg_ratings_per_day\n",
        "WHERE\n",
        "  (branch, avg_rating) IN (\n",
        "    SELECT\n",
        "      branch,\n",
        "      MAX(avg_rating)\n",
        "    FROM\n",
        "      avg_ratings_per_day\n",
        "    GROUP BY\n",
        "      branch\n",
        "  )\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "# Execute the query\n",
        "highest_ratings_per_branch_df = pd.read_sql(query_highest_ratings_per_branch, connection)\n",
        "print(\"Time of Day with the Highest Customer Ratings for Each Branch:\")\n",
        "print(highest_ratings_per_branch_df)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2a77c181",
      "metadata": {
        "id": "2a77c181"
      },
      "source": [
        "\n",
        "\n",
        "### Recommendations\n",
        "\n",
        "1. **Targeted Promotions in High-Performing Branches**: Since Yangon and Naypyitaw branches show strong performance in terms of customer segmentation and purchase value, focus promotional efforts on these locations to drive even higher sales and customer traffic.\n",
        "\n",
        "2. **Optimize Operations in Mandalay**: Mandalay branch shows moderate performance. Investigate operational efficiency, customer experience, and product assortment to find opportunities for improvement. Consider special events or promotions to boost foot traffic.\n",
        "\n",
        "3. **Leverage Peak Times and Days**: With afternoons and Saturdays being the busiest, plan staffing, inventory, and marketing efforts accordingly. Use targeted advertising to attract more customers during these peak times.\n",
        "\n",
        "4. **Expand Payment Options**: Given the popularity of E-wallets among both Member and Normal customers, ensure that all branches support this payment method. Consider additional payment options to meet diverse customer preferences.\n",
        "\n",
        "5. **Enhance Customer Loyalty Programs**: Since Members bring in more revenue but have lower average ratings, revamp loyalty programs to improve their experience. This could include exclusive discounts, events, or early access to new products.\n",
        "\n",
        "### Conclusion\n",
        "The analysis of customer, sales, and product data reveals key trends and areas for improvement. By focusing on high-performing branches, optimizing operations in moderate-performing ones, leveraging peak times, expanding payment options, and enhancing customer loyalty programs, businesses can increase sales, improve customer satisfaction, and drive overall growth.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "32ff7684",
      "metadata": {
        "id": "32ff7684"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}